---
layout: post
title:  "Mantis: Interleaved Multi-Image Instruction Tuning"
date:   2024-04-12 02:21:59 -0400
author: Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, Wenhu Chen
# categories: jekyll update
---


![Figure 1: Mantis]({{"/assets/Mantis/images/overall_barchart.jpeg" | relative_url }})

# Introduction

While many previous works have focused on single-image reasoning tasks, the ability to reason over multiple images is still an open challenge. **Can we enhance large multimodal models with multi-image reasoning ability via instruction tuning?**. To achieve this goal, we propose **Mantis**, a series of LMMs with decent multi-image reasoning ability, achieving SOTA performance on a series of multi-image benchmarks. Mantis is trained on the newly curated dataset **Mantis-Instruct**, a large-scale multi-image QA dataset that covers various multi-image reasoning tasks.

[Resources](#resources) are listed at the end of the blog.

# Methodology

We have modifed the Fuyu and LLaVA's training and inference codes to support **interleaved text-image inputs**. In this version, we use LLaVA-1.5 as the base model for the first version of Mantis, called **Mantis-llava**. (Mantis-Fuyu will be released in the future.)

Similar to LLaVA, we use `<image>` as the placeholder of each image in the text. Besides, for each `<image>` token, we will automatically replace all the images into the following format: `(image {i}: <Image><image></Image>)`, where `i` is the serial number in the sequence of images. 

# Training: Mantis-Instruct dataset

![Figure 2: Illustrations of parts of the datasets in Mantis-Instruct]({{"/assets/Mantis/images/mantis-instruct-cases.jpeg" | relative_url }})

We curate datasets from multiple publicly available datasets to satisfy the training requirements above, forming Mantis-Instruct. The dataset is divided into 4 parts:

1. Multi-Image Reasoning.
The first parts comes from existing datasets where multiple images are involved, including Spot-the-diff, Birds-to-words, Dreamsim, NLVR2. These dataset all involve reasoning across multiple images.

2. Contrastive Captioning.
The second part is the newly curated dataset from existing captioning datasets, ShareGPT-4V, and LAION-GPT4-Vision. Give multiple images, the model need to generate a caption for a specific image (`Caption Generation`), or judge which image matches the provided caption (`Caption Matching`).

3. Multi-Image Instruction Following.
The third part reformats previous single-image multi-turn instruction-following datasets into a multi-image version, including LLaVA-665k-merged, LRV-Instruction-merged. We add proper image denotation like `For the second image, ...` for each question. 

4. Single-Image Reasoning.
Similar to LLaVA-NeXT, we include DocVQA, DVQA, and ChartQA to enhance the model's ability on diagrams and OCR. This is also to avoid the model forgetting ability on single-image reasoning.

# Evaluation: Multi-image evaluation set

We release 2 versions of Mantis, [Mantis-LLaVA-7b](https://huggingface.co/TIGER-Lab/Mantis-llava-7b) and [Mantis-bakLLaVA-7b](https://huggingface.co/TIGER-Lab/Mantis-bakLLaVA-7b). They are all trained on the Mantis-Instruct dataset and part of the [Co-Instruct](https://co-instruct.github.io/) dataset. They are based on [llava-hf/llava-1.5-7b-hf](https://huggingface.co/llava-hf/llava-1.5-7b-hf) and [llava-hf/bakLlava-v1-hf](https://huggingface.co/llava-hf/bakLlava-v1-hf) respectively. Due to the limitation of the backbone, Mantis-LLaVA-7b is trained with a maximum context length of 4096, while Mantis-bakLLaVA-7b is trained with a maximum context length of 8192.

We evaluate the models on multiple benchmarks, including:

1. [NLVR2](https://arxiv.org/abs/1811.00491): Binary Question Answer
2. [Birds-to-words](https://arxiv.org/abs/1909.04101): Multi-choice Question Answering
3. [Qbench2](https://arxiv.org/abs/2309.14181): Binary Question Answering
4. [Mementos](https://arxiv.org/abs/2401.10529): Multi-image captioning
5. Mantis-eval: Multi-choice Question Answering curated by ourselves 

### NLVR2, Birds-to-words, and Mantis-eval (Held-out)

| Models                | NLVR2     | Birds-to-words | Mantis-eval |
|-----------------------|-----------|----------------|------------|
| Random                | 48.93     | 31.16          | 23.04      |
| Blip-2                | 59.42     | 45.10          | 49.77      |
| InstructBlip          | 60.26     | 42.43          | 45.62      |
| QwenVL                | 58.72     | 31.45          | 39.17      |
| Llava                 | 53.88     | 36.20          | 31.34      |
| Kosmos2               | 49.00     | 31.75          | 30.41      |
| fuyu                  | 51.10     | 28.19          | 27.19      |
| Mantis-LLaVA-7b  | **82.00** | **52.23**      | 46.08      |
| Mantis-bakLLaVA-7b  | 82.98     | 47.48          | **50.69**  |

*Table 1: The performance of Mantis-LLaVA-7b and Mantis-bakLLaVA-7b on NLVR2, Birds-to-words, and Mantis-eval.*

As shown in Table 1, our model achieves decent performance on the benchmarks. Specifically, Mantis-LLaVA-7b gets 82.00 on NLVR2 and 52.23 accuracies on Birds-to-words. Mantis-bakLLaVA-7b gets 50.69 on the Mantis-eval dataset, which surpasses the LLaVA-1.5 by 19.35 and BLIP2 by 0.92.

### Qbench2 (Held-out)

| Models                        | Accuracy   |
|-------------------------------|------------|
| GPT-4V (Close-Source)         | **76.5**  |
| Gemini-Pro (Close-Source)     | 57.6      |
| InfiMM (Zephyr-7B)            | 42.9      |
| Fuyu-8B (Persimmon-8B)        | 49.1      |
| mPLUG-Owl2 (LLaMA-7B)         | 49.8      |
| LLaVA-v1.5 (Vicuna-v1.5-7B)   | 49.3      |
| LLaVA-v1.5 (Vicuna-v1.5-13B)  | 49.8      |
| Emu2-Chat (LLaMA-33B)         | 50.0      |
| mPLUG-Owl2 (Q-Instruct)       | 50.5      |
| BakLLava (Mistral-7B)         | 50.9      |
| Qwen-VL-Plus (Close-Source)   | 60.7      |
| Qwen-VL-Max (Close-Source)    | 67.2      |
| Mantis-LLaVA-7b               | 68.7      |
| Mantis-bakLLaVA-7b            | **72.0**  |

*Table 2: The performance of Mantis-LLaVA-7b and Mantis-bakLLaVA-7b on Qbench2-A1-pair-dev evaluation.*

QBench is a benchmark evaluating whether LMMs can properly judge and compare the quality of a benchmark.
On the Qbench2 leaderboard, our model achieves 52.30 accuracy, which surpasses all previous open-source LMMs reported in the leaderboard. After introducing the co-instruct training data, the performance further increases to 72.00 accuracy, which surpasses 3 close-source models and is only behind GPT-4V by 4.52. However, there are some performance decreases on the 2 held-in datasets NLVR2 and Birds-to-words. We attribute it to be the trade-off across various datasets. Besides, the performance on the held-in evaluation dataset after the decreases of our model still surpasses the baseline models for the NLVR2 dataset. For Birds-to-words evaluation datasets, our model is only behind BLIP2 by 2.08.

### Mementos (Held-out)

| Models                        | Daily-Life   |   Robotics   |  Comics |    Avg     |
|-------------------------------|--------------|--------------|---------|------------|
| GPT-4V                        | 33.5         |   33.9       |  18.1   |  **28.50** |
| Gemini                        | 21.6         |   39.4       |  16.3   |  25.77     |
| Video-LLaMA-2                 | 20.1         |   11.2       |  6.8    |  12.70     |
| Chat-UniVi                    | 24.9         |   21.1       |  11.9   |  19.30     |
| LLaVa-1.5                     | 26.6         |   17.9       |  10.2   |  18.23     |
| MiniGPT-4                     | 19.4         |   7.7        |  7.9    |  11.67     |
| MiniGPT-5                     | 19.8         |   5.4        |  9.9    |  11.70     |
| mPLUG-Owl-v2                  | 22.1         |   19.5       |  11.7   |  17.77     |
| InstructBLIP                  | 24.5         |   22.7       |  8.9    |  18.70     |
| Mantis-bakLLaVA-7b            | 29.8         |   32.4       |  16.0   |  **26.07** |

*Table 3: The performance of Mantis-bakLLaVA-7b on Mementos behavior recognition evaluation. (F1-score)*


Mementos is a benchmark to test multiple image sequence reasoning ability. It evaluates whether LMMs can accurately capture the contents in the image sequences, understand the situations, and then generate a detailed description of the provided image sequence. The evaluation will require GPT-4 to extract an action list and an object list described in the generated text, then compare with the reference description with precision, recall, and F1-score.

We report the performance in 3 domains daily life, robotics, and comics. Results are shown in Table 3. Mantis-bakLLaVA-7b achieves 29.80%, 32.38%, and 15.99% F1-score on the daily life, robotics, and comics domains, respectively. Our model surpasses all the previous open-source LMMs. The performance in the daily life domain is only behind GPT-4. We found that Mantis is particularly good at capturing information from dynamic scenes where multiple images are involved. This phenomenon is akin to the insect mantis, which is adept at recognizing moving and dynamic objects but struggles with static objects. This similarity is why we have named our model after this insect.

### Single-Image Reasoning

We also evaluate the performance of Mantis-LLaVA-7b on single-image reasoning tasks, including AI2D, GQA, InfoVQA, MME-C(ognition), MME-P(ception), MMMU, OKVQA, TextVQA. The results are shown in Figure 1. After training on the Mantis-Instruct dataset, Mantis-LLaVA maintains the performance on single-image reasoning tasks compared to its base model LLaVA-1.5-7b. Surprisingly, the performance on the AI2D, InfoVQA, MME-C, MMMU, and TextVQA all get significant improvements. 

# Ongoing Work

Mantis is a active work in progres. We have demonstrated that Mantis-bakLLaVA-7b has achieved remarkable performance on various benchmarks, including NLVR2, Birds-to-words, Mementos, and Qbench2. However, there are still some limitations and future directions that we need to address, such as performance drops on single-image reasoning tasks, and the context length limitation of the model. We plan to keep improving the model's performance on single-image reasoning tasks and explore more efficient ways to handle multiple images. Larger models and more diverse datasets will be used to further improve the model's performance.
We also plan to investigate the effects of the heuristics we have applied in the data curation process and further improve the model's performance on multi-image reasoning tasks. We hope that our work can inspire more research in the field of multi-image reasoning and contribute to the development of large multimodal models.

# Resources

- [Code](https://github.com/TIGER-AI-Lab/Mantis)
- ðŸ¤— [Demo](https://huggingface.co/spaces/TIGER-Lab/Mantis)
- ðŸ¤— [Mantis-LLaVA-7b](https://huggingface.co/TIGER-Lab/Mantis-llava-7b)
- ðŸ¤— [Mantis-bakLLaVA-7b](https://huggingface.co/TIGER-Lab/Mantis-bakllava-7b)
- ðŸ¤— Mantis-Instruct (will be released soon)
