---
layout: post
title:  "Mantis: Multi-Image Instruction Tuning"
date:   2024-04-12 02:21:59 -0400
author: Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, Wenhu Chen
# categories: jekyll update
---

## Introduction
The recent emergence of Large Multimodal Models (LMMs) such as GPT-4V[^1] and Gemini[^2] has shown strong visual-language understanding and generation capabilities in many areas, like image captioning and visual question answering. Despite obtaining the remarkable performance on most visual-language datasets, their ability to reason about multiple images has yet to be explored. Recently, there has been relatively fewer work in exploring LMMs' capability to reason over multiple images. Recently, Mementos[^3] has attempted to benchmark LMMs' ability to comprehend image sequences and observed that the recent LMMs besides GPT-4V are achieving F1 score below 40%. In this work, we aim to enhance LMMs' capability to reason over multiple images, which include different scenarios: (1) understanding actions in multiple video frames, (2) understanding differences between images, (3) high-level reasoning over multiple images.

To achieve these goals, we propose **Multi-Image Instruction Tuning (MIIT)** inspired by the LLaVA's visual instruction tuning [^4]. We aim to extend existing single-image LMM's ability to multiple-image scenarios, which we hypothesize should be achieved with properly curated instruction-tuning datasets, without the need to train a new LMM from scratch. Incorporating multiple images in the training dataset also implicitly makes contrastive learning across images and texts possible, which has been proven to be a simple yet effective method for enhancing video understanding ability. 

In short, our contributions to this paper can be listed as follows:
1. **We propose MIIT**, a new training paradigm for LMMs to equip them with multi-image understanding and reasoning ability via multi-image instruction tuning.
2. **We open-source the training and inference codes**, which support free-form text-image interleaved format to facilitate the development of multi-image LMMs in the community.
3. **We release Mantis-Instruct**, a large-scale multi-image QA dataset curated from current publicly available datasets, which is used for `methodname`'s instruction tuning.
4. **We announce Mantis**, a series of LMMs with decent multi-image reasoning ability based on Llava, achieving SOTA performance on a series of multi-image benchmarks.

## Methodology

To train a model with multi-image reasoning ability, the model structure should be able to support embedding image patches with text token embeddings. Fuyu is a decoder-only LMM with no separate image encoder, which processes image tokens the same as text tokens. Therefore, it can naturally be modified with just the training and inference codes to support interleaved image inputs. However, due to Fuyu's high-resolution feature, the number of image tokens increases rapidly as the image gets larger, with a maximum of `1920x1080 / 30^2 = 2304` for each image, thus demanding a high context length during the training and inference. Besides Fuyu, we also consider LLaVA, which uses a CLIP encoder to first transform every image into a fixed number of image tokens, which is `(336/14)^2=576` for LLaVa-1.5. Since LLaVA-1.5's max positional embedding size is 4096, it can accept about 7 images at most, which is acceptable considering our goals and GPU resources. Therefore, we have decided to use LLaVA-1.5 as the base model for the first version of Mantis, called **Mantis-llava**. (Mantis-Fuyu will be released in the future.)

Besides the model structure consideration, it's also important to design a proper data format for our model. Similar to LLaVA, we use `<image>` as the placeholder of each image in the text, indicating where the image tokens will be inserted. However, in the multi-image scenario, the boundaries between images and the serial order of the images are not explicitly specified, thus making it hard for the model to learn it implicitly through the position embeddings. This ability decides whether the model can clarify the relationship between text denotation for the image and the actual image patches denoted. Therefore, for each `<image>` token, we will automatically replace all the images into the following format: `(image {i}: <Image><image></Image>)`, where `i` is the serial number in the sequence of images. The advantage of this technique is to build a proper attention mapping from the text image denotation to the actual image embeddings, making the learning process easier.

## Mantis-Instruct: A large scale Multi-Image Question Answering dataset

We curate datasets from multiple publicly available datasets to satisfy the training requirements above, forming Mantis-Instruct. We use a similar dataset format with LLaVA's, where the difference is that each data item contains multiple images. Multiple turns of QA pairs are gathered for each item.

Mantis-Instruct can mainly be divided into 3 parts. 

**1. Multi-Image Reasoning.**
The first parts comes from existing datasets where multiple images are involved, including Spot-the-diff, Birds-to-words, Dreamsim, NLVR2. These dataset all involve reasoning across multiple images, including tasks like difference description, image similarity judgment, and logical reasoning across images. We format Spot-the-diff and Birds-to-words into a long-answer QA format, while Dreamsim and NLVR2 are formatted into a multi-choice QA format.

**2. Contrastive Captioning.**
The second part is the newly curated dataset, including Contrastive Captioning. Since the original image captioning tasks are single-image tasks, we reformat them into a multi-image version. After providing multiple images, we define 2 kinds of tasks, called `Caption Matching` and `Caption Generation`. For the `Caption Matching` task, the model needs to judge which image matches the provided caption. For the `Caption Generation` task, the model needs to generate a caption for a denoted image, like image 2. We randomly select 2 to 8 images along with their captions to form a single item, then apply the template of the 2 tasks defined.

**3. Multi-Image Instruction Following.**
The third part reformats previous single-image multi-turn instruction-following datasets into a multi-image version, including LLaVA-665k-merged, LRV-Instruction-merged. We randomly merge 2 to 4 original data items of a single image into a multi-image data item. Then add proper image denotation like `For the second image, ...` for each question. 

**4. Single-Image Reasoning.**
Instead of focusing only on multi-image tasks, we claim that it's necessary to also include some single-image VQA datasets to avoid the model forgetting ability on single-image reasoning. We included some of the single image data from LLaVA-665k. Inspired by LLaVA-NeXT, we also add DocVQA, DVQA, and ChartQA to enhance its ability on diagrams and OCR. In practice, since the size of DVQA is too large, we only use 30k output 200k of it for the training.

## Experiment Results

We release 2 versions of Mantis, Mantis-llava-7b-v1.0 and Mantis-llava-7b-v1.1. Mantis-llava-7b-v1.0 is trained on the Mantis-Instruct dataset, while Mantis-llava-7b-v1.1 is trained on the Mantis-Instruct dataset along with Co-Instruct[^6] training data. They are based on [llava-hf/llava-1.5-7b-hf](https://huggingface.co/llava-hf/llava-1.5-7b-hf) and [llava-hf/bakLlava-v1-hf](https://huggingface.co/llava-hf/bakLlava-v1-hf) respectively. We evaluate the models on multiple benchmarks, including NLVR2, Birds-to-words, Mantis-Instruct-eval, Qbench2, and Mementos.

### Evaluation on NLVR2, Birds-to-words, and Mantis-Instruct-eval

| Models          | NLVR2 | Birds-to-words | Mantis-Instruct-eval |
|-----------------|-------|----------------|------------|
| Random          | 48.93 | 31.16          | 23.04      |
| Blip-2          | 59.42 | 45.70          | 49.77      |
| InstructBlip    | 60.26 | 42.43          | 45.62      |
| QwenVL          | 58.72 | 31.45          | 39.17      |
| Llava           | 53.88 | 36.20          | 31.34      |
| Kosmos2         | 49.00 | 31.75          | 30.41      |
| fuyu            | 51.10 | 28.19          | 27.19      |
| Mantis-llava-7b-v1.0  | **84.93** | **52.52**  | 45.16      |
| Mantis-llava-7b-v1.1  | 82.98 | 43.62          | **50.69**  |

*Table 1: The performance of Mantis-llava-7b-v1.0 and Mantis-llava-7b-v1.1 on NLVR2, Birds-to-words, and Mantis-Instruct-eval evaluation.*

We reformat the test set of NLVR2 and Birds-to-words into a multi-choice format, then evaluate the models on the Mantis-Instruct-eval evaluation dataset. We curated 200+ multi-image inference examples that cover different topics. 
The images are collected by our annotators via Google Search, and then come up with a proper question for the chosen topics. They are all multi-choice questions. For those models that cannot accept multiple images as input, we merge the images into a single one, putting them side by side.

As shown in Table 1, our model achieves decent performance on the benchmarks. Specifically, Mantis-llava-7b-v1.0 gets 84.93 on NLVR2 and 52.52 accuracies on Birds-to-words. Mantis-llava-7b-v1.1 gets 50.69 on the Mantis-Instruct-eval subset, which surpasses the LLaVA-1.5 by 19.35 and BLIP2 by 0.92.

### Evaluation on Qbench2

| Models                        | Acc   |
|-------------------------------|-------|
| InfiMM (Zephyr-7B)            | 42.95 |
| Emu2-Chat (LLaMA-33B)         | 50.05 |
| Fuyu-8B (Persimmon-8B)        | 49.15 |
| BakLLava (Mistral-7B)         | 50.94 |
| mPLUG-Owl2 (Q-Instruct)       | 50.54 |
| mPLUG-Owl2 (LLaMA-7B)         | 49.85 |
| LLaVA-v1.5 (Vicuna-v1.5-7B)   | 49.32 |
| LLaVA-v1.5 (Vicuna-v1.5-13B)  | 49.85 |
| Qwen-VL-Plus (Close-Source)   | 60.70 |
| Qwen-VL-Max (Close-Source)    | 67.27 |
| Gemini-Pro (Close-Source)     | 57.64 |
| GPT-4V (Close-Source)         | 76.52 |
| Mantis-llava-7b-v1.0                | 52.30 |
| Mantis-llava-7b-v1.1                | **72.00** |

*Table 2: The performance of Mantis-llava-7b-v1.0 and Mantis-llava-7b-v1.1 on Qbench2-A1-single-dev evaluation.*

QBench is a benchmark evaluating whether LMMs can properly judge and compare the quality of a benchmark.
On the Qbench2 leaderboard, our model achieves 52.30 accuracy, which surpasses all previous open-source LMMs reported in the leaderboard. After introducing the co-instruct training data, the performance further increases to 72.00 accuracy, which surpasses 3 close-source models and is only behind GPT-4V by 4.52. However, there are some performance decreases on the 2 held-in datasets NLVR2 and Birds-to-words. We attribute it to be the trade-off across various datasets. Besides, the performance on the held-in evaluation dataset after the decreases of our model still surpasses the baseline models for the NLVR2 dataset. For Birds-to-words evaluation datasets, our model is only behind BLIP2 by 2.08.

### Evaluation on Mementos

![Table 3: The performance of Mantis-llava-7b-v1.1 on Mementos behavior recognition evaluation.]({{"/assets/Mantis/images/mementos.jpeg" | relative_url }})

Mementos is a benchmark to test multiple image sequence reasoning ability. It evaluates whether LMMs can accurately capture the contents in the image sequences, understand the situations, and then generate a detailed description of the provided image sequence. The evaluation will require GPT-4 to extract an action list and an object list described in the generated text, then compare with the reference description with precision, recall, and F1-score.

We report the performance in 3 domains daily life, robotics, and comics. Results are shown in Table 3. Mantis-llava-7b-v1.1 achieves 29.80%, 32.38%, and 15.99% F1-score on the daily life, robotics, and comics domains, respectively. Our model surpasses all the previous open-source LMMs. The performance in the daily life domain is only behind GPT-4. We found that Mantis is particularly good at capturing information from dynamic scenes where multiple images are involved. This phenomenon is akin to the insect mantis, which is adept at recognizing moving and dynamic objects but struggles with static objects. This similarity is why we have named our model after this insect.

## Future Work

Mantis is a active work in progres. We have demonstrated that Mantis-llava-7b-v1.1 has achieved remarkable performance on various benchmarks, including NLVR2, Birds-to-words, Mementos, and Qbench2. However, there are still some limitations and future directions that we need to address, such as performance drops on single-image reasoning tasks, and the context length limitation of the model. We plan to keep improving the model's performance on single-image reasoning tasks and explore more efficient ways to handle multiple images. Larger models and more diverse datasets will be used to further improve the model's performance.
We also plan to investigate the effects of the heuristics we have applied in the data curation process and further improve the model's performance on multi-image reasoning tasks. We hope that our work can inspire more research in the field of multi-image reasoning and contribute to the development of large multimodal models.

## References

[^1]: Achiam, J. (2023). GPT-4 Technical Report.
[^2]: Team, C. (2023). Gemini: A Family of Highly Capable Multimodal Models.
[^3]: Wang, L. (2024). Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences.
[^4]: Liu et al. (2023), Visual Instruction Tuning.
[^5]: Liu et al. (2023), Improved Baselines with Visual Instruction Tuning
[^6]: Wu et al. (2023), Towards Open-ended Visual Quality Comparison
<!-- [^9]: Li et al. (2023). MIMIC-IT
[^10]: Forbes et al. (2019). Neural Network Generation of Questions for Question Answering.
[^11]: Fu et al. (2023). DreamSim: Simulated Dreams and Learning.
[^12]: Suhr et al. (2018). NLVR2: Understanding the role of images in model training.
[^13]: Chen et al. (2023). ShareGPT-4V: Sharing Visual-Language Models.
[^14]: LAION (2023). LVIS: Large Vocabulary Instance Segmentation.
[^15]: Wu et al. (2023). QBench: A Benchmark for Question Answering.
[^16]: Xiao et al. (2021). NExTQA: A Benchmark for Video Question Answering.
[^17]: Wu et al. (2021). STAR: A Benchmark for Video Question Answering.
[^18]: Huang et al. (2016). Visual Storytelling.
[^19]: Ko et al. (2023). Flipped-VQA:  -->
