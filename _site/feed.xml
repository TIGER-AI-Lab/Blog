<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-04-12T03:34:35-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">TIGER-AI-Lab</title><subtitle>We are a dynamic team of dedicated researchers and faculty members. Our team is constantly growing and we welcome individuals who share our passion for advancing GenAI.</subtitle><entry><title type="html">Mantis-VL</title><link href="http://localhost:4000/jekyll/update/2024/04/12/mantis-vl.html" rel="alternate" type="text/html" title="Mantis-VL" /><published>2024-04-12T02:21:59-04:00</published><updated>2024-04-12T02:21:59-04:00</updated><id>http://localhost:4000/jekyll/update/2024/04/12/mantis-vl</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/04/12/mantis-vl.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The recent emergence of Large Multimodal Models (LMMs) such as GPT-4V<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> and Gemini<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> has shown strong visual-language understanding and generation capabilities in many areas, like image captioning and visual question answering. Despite obtaining the remarkable performance on most visual-language datasets, their ability to reason about multiple images has yet to be explored. Recently, there has been relatively fewer work in exploring LMMs’ capability to reason over multiple images. Recently, Mementos<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> has attempted to benchmark LMMs’ ability to comprehend image sequences and observed that the recent LMMs besides GPT-4V are achieving F1 score below 40%. In this work, we aim to enhance LMMs’ capability to reason over multiple images, which include different scenarios: (1) understanding actions in multiple video frames, (2) understanding differences between images, (3) high-level operations over multiple images.</p>

<h2 id="miit-multi-image-instruction-tuning">MIIT: Multi-Image Instruction Tuning</h2>

<h3 id="methodology">Methodology</h3>
<p>To train a model with multi-image reasoning ability, the model structure should be able to support embedding image patches with text token embeddings. Fuyu is a decoder-only LMM with no separate image encoder, which processes image tokens the same as text tokens. Therefore, it can naturally be modified with just the training and inference codes to support interleaved image inputs. However, due to Fuyu’s high-resolution feature, the number of image tokens increases rapidly as the image gets larger, with a maximum of <code class="language-plaintext highlighter-rouge">1920x1080 / 30^2 = 2304</code> for each image, thus demanding a high context length during the training and inference. Besides Fuyu, we also consider LLaVA, which uses a CLIP encoder to first transform every image into a fixed number of image tokens, which is <code class="language-plaintext highlighter-rouge">(336/14)^2=576</code> for LLaVa-1.5. Since LLaVA-1.5’s max positional embedding size is 4096, it can accept about 7 images at most, which is acceptable considering our goals and GPU resources. Therefore, we have decided to use LLaVA-1.5 as the base model for our experiments.</p>

<p>Besides the model structure consideration, it’s also important to design a proper data format for our model. Similar to LLaVA, we use <code class="language-plaintext highlighter-rouge">&lt;image&gt;</code> as the placeholder of each image in the text, indicating where the image tokens will be inserted. However, in the multi-image scenario, the boundaries between images and the serial order of the images are not explicitly specified, thus making it hard for the model to learn it implicitly through the position embeddings. This ability decides whether the model can clarify the relationship between text denotation for the image and the actual image patches denoted. Therefore, for each <code class="language-plaintext highlighter-rouge">&lt;image&gt;</code> token, we will automatically replace all the images into the following format: <code class="language-plaintext highlighter-rouge">(image {i}: &lt;Image&gt;&lt;image&gt;&lt;/Image&gt;)</code>, where <code class="language-plaintext highlighter-rouge">i</code> is the serial number in the sequence of images. The advantage of this technique is to build a proper attention mapping from the text image denotation to the actual image embeddings, making the learning process easier.</p>

<p>Similar to visual instruction tuning<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>, data curation is an important step to ensure good performance. During the curation, we have found some heuristics that can improve the performance. We list them below. These heuristics are applied in the curation of the dataset, which is described in further detail elsewhere<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>.</p>

<ol>
  <li><strong>Conflicts between short answers and long answers.</strong> Many datasets we collected are domain-specific, and some of them are classification tasks like NLVR2<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup> and DreamSim<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote" rel="footnote">7</a></sup>. For these datasets, we convert them into multiple-choice QA format to avoid conflicts between repeated short-answers and some long answers from other datasets.</li>
  <li><strong>Addition of image denotation in the text for each question.</strong>
We manually add image denotations like “the first image”, “image 2”, “the right image”, etc. to each question in our dataset to make sure the question involved with multiple images is clear to answer.</li>
  <li><strong>Positions of the <code class="language-plaintext highlighter-rouge">&lt;image&gt;</code> placeholder.</strong>
For each item with multiple images, we write some rules to randomly put the image placeholders at the beginning of the first question or the end of the first question. This technique is also used by the curation LLaVA-665k<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote" rel="footnote">8</a></sup>.</li>
</ol>

<p>For now, the effects of these heuristics are not well-investigated and quantized due to the limited time and resources. We aim to investigate further in the future work.</p>

<h3 id="miqa-a-large-scale-multi-image-question-answering-dataset">MIQA: A large scale Multi-Image Question Answering dataset</h3>

<table>
  <thead>
    <tr>
      <th>Subsets</th>
      <th># Examples (k)</th>
      <th># Avg image</th>
      <th># Max image</th>
      <th># Avg Turns</th>
      <th>Avg Length(T)</th>
      <th>Avg Length (T&amp;I)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>NLVR2</td>
      <td>86.37</td>
      <td>2.00</td>
      <td>2</td>
      <td>2.00</td>
      <td>105</td>
      <td>1257</td>
    </tr>
    <tr>
      <td>Birds-to-words</td>
      <td>2.65</td>
      <td>2.00</td>
      <td>2</td>
      <td>2.00</td>
      <td>101</td>
      <td>1253</td>
    </tr>
    <tr>
      <td>Dreamsim</td>
      <td>15.94</td>
      <td>3.00</td>
      <td>3</td>
      <td>2.00</td>
      <td>103</td>
      <td>1831</td>
    </tr>
    <tr>
      <td>Contrastive Captioning</td>
      <td>35.98</td>
      <td>3.81</td>
      <td>8</td>
      <td>7.62</td>
      <td>871</td>
      <td>3067</td>
    </tr>
    <tr>
      <td>Spot-the-diff</td>
      <td>8.01</td>
      <td>2.00</td>
      <td>2</td>
      <td>3.99</td>
      <td>121</td>
      <td>1273</td>
    </tr>
    <tr>
      <td>Llava-665k-merged</td>
      <td>312.61</td>
      <td>1.99</td>
      <td>4</td>
      <td>21.44</td>
      <td>558</td>
      <td>1710</td>
    </tr>
    <tr>
      <td>LRV-merged</td>
      <td>8.45</td>
      <td>3.50</td>
      <td>9</td>
      <td>83.37</td>
      <td>2234</td>
      <td>4251</td>
    </tr>
    <tr>
      <td>Nexqa</td>
      <td>3.87</td>
      <td>8.00</td>
      <td>8</td>
      <td>17.64</td>
      <td>572</td>
      <td>5180</td>
    </tr>
    <tr>
      <td>Star</td>
      <td>3.03</td>
      <td>8.00</td>
      <td>8</td>
      <td>30.16</td>
      <td>961</td>
      <td>5569</td>
    </tr>
    <tr>
      <td>Visual story telling</td>
      <td>6.66</td>
      <td>20.33</td>
      <td>50</td>
      <td>9.71</td>
      <td>530</td>
      <td>12238</td>
    </tr>
    <tr>
      <td>Coinstruct</td>
      <td>150.92</td>
      <td>2.67</td>
      <td>4</td>
      <td>7.45</td>
      <td>314</td>
      <td>1620</td>
    </tr>
    <tr>
      <td>Dvqa</td>
      <td>200.00</td>
      <td>1.00</td>
      <td>1</td>
      <td>40.00</td>
      <td>304</td>
      <td>880</td>
    </tr>
    <tr>
      <td>Docvqa</td>
      <td>39.46</td>
      <td>1.00</td>
      <td>1</td>
      <td>2.00</td>
      <td>62</td>
      <td>638</td>
    </tr>
    <tr>
      <td>Chartqa</td>
      <td>28.30</td>
      <td>1.00</td>
      <td>1</td>
      <td>2.00</td>
      <td>66</td>
      <td>642</td>
    </tr>
  </tbody>
</table>

<p><em>Table 1: The statistics of MIQA datasets. We list the number of examples, average images per example, maximum image per example, average turns, average text token lengths, and average text+image token lengths. Here we use Llava’s tokenizer and assume each image occupies 576 image tokens, which is the number of tokens each image will be converted to in Llava model. Here “Avg Length(T)” denotes the average number of tokens for the text part, while “Avg Length(T&amp;I)” means the average number of tokens of text and images in total.</em></p>

<p>We curate datasets from multiple publicly available datasets to satisfy the training requirements above, forming MIQA. Detailed statistics of MIQA are shown in Table 1. We use a similar dataset format with LLaVA’s, where each data item contains multiple images and multiple turns of QA pairs are gathered. We report the number of examples, the number of images per item, the average conversation turns, and the average length.</p>

<p>We describe the task type of each dataset and the processing techniques used in the followings:</p>

<h4 id="spot-the-diff">Spot-the-diff</h4>
<p>This is a dataset which requires the model to generate the difference description given 2 images. We use the processed version from MIMIC-IT<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>.</p>

<h4 id="birds-to-words">Birds-to-words</h4>
<p>The task of this dataset is to generate different descriptions and give 2 images of different birds. We directly take the training split for training. Besides, we prompt GPT-3.5-turbo to generate a possible question given the reference answer, so we can get a diverse question pool<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote" rel="footnote">10</a></sup>.</p>

<h4 id="dreamsim">Dreamsim</h4>
<p>DreamSim proposes a dataset to train a model to judge image similarity. We reuse their datasets, and the task is to judge which image of the 2 candidate images is more similar to the given reference image<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote" rel="footnote">11</a></sup>.</p>

<h4 id="nlvr2">NLVR2</h4>
<p>NLVR2 is a natural language reasoning dataset grounded in images. The task is to determine whether a statement is true or false given the visual contents of 2 images. NLVR2 covers many linguistic phenomena, such as cardinality, existential quantifiers, universal quantifiers, etc.<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote" rel="footnote">12</a></sup>.</p>

<h4 id="contrastive-captioning">Contrastive captioning</h4>
<p>Inspired by the paradigm of contrastive learning, we reformat existing image captioning tasks into a multi-image version, where 2 tasks are defined. Given multiple images, the first task we defined is to judge which image matches the provided caption, and the second task is to generate a caption for a denoted image. We randomly select 2 to 8 images along with their captions to form a single item, then apply the template of the 2 tasks defined. We used the images and captions provided by ShareGPT-4V<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote" rel="footnote">13</a></sup> and LAION GPT-4V captions from LVIS<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote" rel="footnote">14</a></sup>.</p>

<h4 id="co-instruct">Co-Instruct</h4>
<p>Co-Instruct is a dataset curated to compare the quality of 2-4 images in the format of open-ended answer or multi-choice QA format. It’s similar to the tasks in the Qbench<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote" rel="footnote">15</a></sup>.</p>

<h4 id="video-qa">Video QA</h4>
<p>We also include some video question answering datasets, including nextqa<sup id="fnref:16" role="doc-noteref"><a href="#fn:16" class="footnote" rel="footnote">16</a></sup>, star<sup id="fnref:17" role="doc-noteref"><a href="#fn:17" class="footnote" rel="footnote">17</a></sup>, and visual-story-telling<sup id="fnref:18" role="doc-noteref"><a href="#fn:18" class="footnote" rel="footnote">18</a></sup>. nextqa and star are the processed version from Flipped-VQA<sup id="fnref:19" role="doc-noteref"><a href="#fn:19" class="footnote" rel="footnote">19</a></sup>. The visual-story-telling are the processed version from MIMIC-IT<sup id="fnref:9:1" role="doc-noteref"><a href="#fn:9" class="footnote" rel="footnote">9</a></sup>.</p>

<h4 id="llava-665k-merged">LLaVA-665k-merged</h4>
<p>Besides the above-mentioned multi-image reasoning tasks, we also manually reformat the LLaVA-655k dataset from LLaVA-1.5 into a “fake” multi-image dataset. We randomly merge 2 to 4 original data items of a single image into a multi-image data item. Then add proper image denotation like “For the second image, …” for each question. Then we shuffle all the conversation pairs to form the final merge items. It’s a “fake” multi-image dataset because answering each question in the conversation does not require reasoning across images. However, due to the quality and diversity of the LLaVA-665k data, we contend that it’s helpful for avoiding forgetting via replaying and increasing instruction-following ability.</p>

<h4 id="lrv-merged">LRV-merged</h4>
<p>This is a single-image dataset used to mitigate hallucinations of LMMs. We process it similarly to LLaVA-665k-merged<sup id="fnref:4:1" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> to get a “fake” multi-image dataset.</p>

<h4 id="single-image-qa">Single-image-QA</h4>
<p>Instead of focusing only on multi-image tasks, we claim that it’s necessary to also include some single-image VQA datasets to avoid the model forgetting ability on single-image reasoning. We included some of the single image data from LLaVA-665k. Inspired by LLaVA-NeXT, we also add DocVQA, DVQA, and ChartQA to enhance its ability on diagrams and OCR. In practice, since the size of DVQA is too large, we only use 30k output 200k of it for the training.</p>

<h3 id="training-details">Training details</h3>
<p>We have trained multiple versions of our models with different dataset-mixing strategies and different backbones. Finally, we select 2 versions called <code class="language-plaintext highlighter-rouge">Mantis-7b-v1.0</code> based on<a href="https://huggingface.co/llava-hf/llava-1.5-7b-hf">LLaVA-1.5-7b-hf</a>, where the training context length is set to 4096, and <code class="language-plaintext highlighter-rouge">Mantis-7b-v1.1</code> based on<a href="https://huggingface.co/llava-hf/bakLlava-v1-hf">bakLlava-v1-hf</a>, where the training context length is set to 8192. <code class="language-plaintext highlighter-rouge">Mantis-7b-v1.0</code> is not trained on coinstrut, while <code class="language-plaintext highlighter-rouge">Mantis-7b-v1.1</code> is trained on all the datasets in <code class="language-plaintext highlighter-rouge">datasetname</code>.</p>

<p>We train each model on the data for 1 epoch, with a batch size of 128. The learning rate is set to 1e-5, which is half of the learning rate used in LLaVA-1.5’s instruction-tuning. We set the warmup ratio to be 0.03 and use a cosine learning rate scheduler. We speed up our training and inference with the help of <a href="https://example.com/dao2023flashattention2">Flash-attention2</a>. The training ran on 16 A100 (40G) GPUs for about 36 hours.</p>

<h2 id="experiments">Experiments</h2>

<h3 id="baselines">Baselines</h3>
<p>For the MIQA-eval evaluation, we have included some famous LMMs like BLIP-2, InstructBlip, Qwen-VL, LLaVA, Kosmos2, and Fuyu. Since these models cannot accept multiple images as input, we deliberately merge the images into a single one, putting them side by side so that they can serve as the baselines.</p>

<p>For Qbench’s evaluation, we include the public leaderboard’s results on the Perception(A1-Pair, dev) of Qbench. For mementos’s evaluation, we include the numbers reported in their paper as baselines for comparison.</p>

<h3 id="evaluation-benchmarks">Evaluation benchmarks</h3>
<p>We mainly report results on 5 benchmarks, where 2 of them are held-in datasets (NLVR2, Birds-to-words), and 3 of them are held-out datasets (Human-eval, Mementos, Q-bench-2).</p>

<h3 id="nlvr2-1">NLVR2</h3>
<p>NLVR2 evaluates the model’s ability to conduct logical reasoning across the contents of images via a judgment of a statement.</p>

<h3 id="birds-to-words-1">Birds-to-words</h3>
<p>Birds-to-Worlds is reformatted into a multi-choice dataset by GPT-3.5-turbo providing the reference difference description, evaluating the model’s ability to identify the difference between 2 images.</p>

<h3 id="human-eval">Human-eval</h3>
<p>We have also curated 200+ multiple-image inference examples that cover different topics, including topics like “Is the <object> in image 1 smaller than the <object> in image 2?”, “Does one image seem to be taken at a different time of day than the other?”, etc. These images are collected by our annotators via Google Search, and then come up with a proper question for the chosen topics. They are all multi-choice questions.</object></object></p>

<h3 id="mementos">Mementos</h3>
<p>Mementos is a benchmark to test multiple image sequence reasoning ability. It evaluates whether LMMs can accurately capture the contents in the image sequences, understand the situations, and then generate a detailed description of the provided image sequence. The evaluation will require GPT-4 to extract an action list and an object list described in the generated text, then compare with the reference description with precision, recall, and F1-score.</p>

<h3 id="qbench">QBench</h3>
<p>QBench is a benchmark evaluating whether LMMs can properly judge and compare the quality of a benchmark. They are all multi-choice questions.</p>

<h2 id="results">Results</h2>
<h3 id="evaluation-on-miqa-eval">Evaluation on MIQA-eval</h3>

<table>
  <thead>
    <tr>
      <th>Models</th>
      <th>NLVR2</th>
      <th>Birds-to-words</th>
      <th>MIQA-eval</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Random</td>
      <td>48.93</td>
      <td>31.16</td>
      <td>23.04</td>
    </tr>
    <tr>
      <td>Blip-2</td>
      <td>59.42</td>
      <td>45.70</td>
      <td>49.77</td>
    </tr>
    <tr>
      <td>InstructBlip</td>
      <td>60.26</td>
      <td>42.43</td>
      <td>45.62</td>
    </tr>
    <tr>
      <td>QwenVL</td>
      <td>58.72</td>
      <td>31.45</td>
      <td>39.17</td>
    </tr>
    <tr>
      <td>Llava</td>
      <td>53.88</td>
      <td>36.20</td>
      <td>31.34</td>
    </tr>
    <tr>
      <td>Kosmos2</td>
      <td>49.00</td>
      <td>31.75</td>
      <td>30.41</td>
    </tr>
    <tr>
      <td>fuyu</td>
      <td>51.10</td>
      <td>28.19</td>
      <td>27.19</td>
    </tr>
    <tr>
      <td>Mantis-7b-v1.0</td>
      <td><strong>84.93</strong></td>
      <td><strong>52.52</strong></td>
      <td>45.16</td>
    </tr>
    <tr>
      <td>Mantis-7b-v1.1</td>
      <td>82.98</td>
      <td>43.62</td>
      <td><strong>50.69</strong></td>
    </tr>
  </tbody>
</table>

<p><em>Table 2: The performance of Mantis-7b-v1.0 and Mantis-7b-v1.1 on NLVR2, Birds-to-words, and MIQA-eval evaluation.</em></p>

<p>We report results on birds-to-words, NLVR2, and our curated human-eval benchmark. As shown in the table, our model achieves decent performance on the benchmarks. Specifically, Mantis-7b-v1.0 gets 84.93 on NLVR2 and 52.52 accuracies on Birds-to-words. Mantis-7b-v1.1 gets 50.69 on the Human-eval subset, which surpasses the LLaVA-1.5 by 19.35 and BLIP2 by 0.92.</p>

<h3 id="evaluation-on-qbench2">Evaluation on Qbench2</h3>

<table>
  <thead>
    <tr>
      <th>Models</th>
      <th>Acc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>InfiMM (Zephyr-7B)</td>
      <td>42.95</td>
    </tr>
    <tr>
      <td>Emu2-Chat (LLaMA-33B)</td>
      <td>50.05</td>
    </tr>
    <tr>
      <td>Fuyu-8B (Persimmon-8B)</td>
      <td>49.15</td>
    </tr>
    <tr>
      <td>BakLLava (Mistral-7B)</td>
      <td>50.94</td>
    </tr>
    <tr>
      <td>mPLUG-Owl2 (Q-Instruct)</td>
      <td>50.54</td>
    </tr>
    <tr>
      <td>mPLUG-Owl2 (LLaMA-7B)</td>
      <td>49.85</td>
    </tr>
    <tr>
      <td>LLaVA-v1.5 (Vicuna-v1.5-7B)</td>
      <td>49.32</td>
    </tr>
    <tr>
      <td>LLaVA-v1.5 (Vicuna-v1.5-13B)</td>
      <td>49.85</td>
    </tr>
    <tr>
      <td>Qwen-VL-Plus (Close-Source)</td>
      <td>60.70</td>
    </tr>
    <tr>
      <td>Qwen-VL-Max (Close-Source)</td>
      <td>67.27</td>
    </tr>
    <tr>
      <td>Gemini-Pro (Close-Source)</td>
      <td>57.64</td>
    </tr>
    <tr>
      <td>GPT-4V (Close-Source)</td>
      <td>76.52</td>
    </tr>
    <tr>
      <td>Mantis-7b-v1.0</td>
      <td>52.30</td>
    </tr>
    <tr>
      <td>Mantis-7b-v1.1</td>
      <td><strong>72.00</strong></td>
    </tr>
  </tbody>
</table>

<p><em>Table 3: The performance of Mantis-7b-v1.0 and Mantis-7b-v1.1 on Qbench2-A1-single-dev evaluation.</em></p>

<p>On the Qbench2 leaderboard, our model achieves 52.30 accuracy, which surpasses all previous open-source LMMs reported in the leaderboard. After introducing the co-instruct training data, the performance further increases to 72.00 accuracy, which surpasses 3 close-source models and is only behind GPT-4V by 4.52. However, there are some performance decreases on the 2 held-in datasets NLVR2 and Birds-to-words. We attribute it to be the trade-off across various datasets. Besides, the performance on the held-in evaluation dataset after the decreases of our model still surpasses the baseline models for the NLVR2 dataset. For Birds-to-words evaluation datasets, our model is only behind BLIP2 by 2.08.</p>

<h3 id="evaluation-on-mementos">Evaluation on Mementos</h3>

<p><img src="/assets/Mantis/images/mementos.jpeg" alt="Table 4: The performance of Mantis-7b-v1.1 on Mementos evaluation." /></p>

<!-- *Table 4: The performance of Mantis-7b-v1.1 on Mementos evaluation.* -->
<p>We also evaluated our model on the Mementos benchmark on the 3 subdomains, daily life, robotics, and comics. Results are shown as follows: For the daily-life domain, our model gets 36.62% F1-score for the object recognition, which is on par with Gemini. Additionally, we get 29.80% F1-score for the behavior recognition, which surpasses all other models except GPT-4. Our precision reaches 38.88% for the daily-life behavior recognition, which is the best across all the models, even better than GPT-4. This demonstrates our model’s great capability to capture dynamic various across multiple image sequences.</p>

<p>Similarly, on the other 2 subdomains, we achieve 32.38% F1-score for behavior recognition on robotics, which is way better than in previous LMMs like LLaVA-1.5 (17.95%), and Chat-UniVi (21.14%). This performance is close to GPT-4’s 33.95%. On the comics domain, our model achieves a 15.99% F1-score for behavior recognition, which surpasses previous LMMs like LLaVA-1.5 by 5.72% and is only behind GPT-4V and Gemini.</p>

<p>While we observe significant performance improvements in behavior recognition, it’s also worth noting that our model, Mantis-7b-v1.1, has experienced some performance drops on the object recognition task compared to the LLaVA-1.5 models. We believe this decline is due to our training datasets being mostly comprised of tasks that require multi-image reasoning ability, thus decreasing the recognition ability for a single static image as a trade-off. This phenomenon is akin to the insect mantis, which is adept at recognizing moving and dynamic objects but struggles with static objects. This similarity is why we have named our model after this insect.</p>

<h2 id="future-work">Future Work</h2>

<h2 id="references">References</h2>

<!-- 
You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

Jekyll requires blog post files to be named according to the following format:

`YEAR-MONTH-DAY-title.MARKUP`

Where `YEAR` is a four-digit number, `MONTH` and `DAY` are both two-digit numbers, and `MARKUP` is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:


<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>


Check out the[Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at[Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on[Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ -->
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Achiam, J. (2023). GPT-4 Technical Report. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Team, C. (2023). Gemini: Understanding visual-language interactions. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Wang, L. (2024). Mementos: Benchmarking the understanding of image sequences. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Liu, Visual Instruction Tuning: Enhancing Visual Models with Instruction-Based Tuning. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:4:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Detailed section in supplementary materials. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Suhr et al. (2018). NLVR2: Understanding the role of images in model training. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>Fu (2023). DreamSim: Simulated Dreams and Learning. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>Liu et al. (2023). Improved Batch Weighting for LLaVA-665k: A Study on Effective Training Techniques. <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>Li et al. (2023). MIMIC-IT <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:9:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>Forbes et al. (2019). Neural Network Generation of Questions for Question Answering. <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>Fu et al. (2023). DreamSim: Simulated Dreams and Learning. <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>Suhr et al. (2018). NLVR2: Understanding the role of images in model training. <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:13" role="doc-endnote">
      <p>Chen et al. (2023). ShareGPT-4V: Sharing Visual-Language Models. <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:14" role="doc-endnote">
      <p>LAION (2023). LVIS: Large Vocabulary Instance Segmentation. <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:15" role="doc-endnote">
      <p>Wu et al. (2023). QBench: A Benchmark for Question Answering. <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:16" role="doc-endnote">
      <p>Xiao et al. (2021). NExTQA: A Benchmark for Video Question Answering. <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:17" role="doc-endnote">
      <p>Wu et al. (2021). STAR: A Benchmark for Video Question Answering. <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:18" role="doc-endnote">
      <p>Huang et al. (2016). Visual Storytelling. <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:19" role="doc-endnote">
      <p>Ko et al. (2023). Flipped-VQA: <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, Wenhu Chen</name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Introduction The recent emergence of Large Multimodal Models (LMMs) such as GPT-4V1 and Gemini2 has shown strong visual-language understanding and generation capabilities in many areas, like image captioning and visual question answering. Despite obtaining the remarkable performance on most visual-language datasets, their ability to reason about multiple images has yet to be explored. Recently, there has been relatively fewer work in exploring LMMs’ capability to reason over multiple images. Recently, Mementos3 has attempted to benchmark LMMs’ ability to comprehend image sequences and observed that the recent LMMs besides GPT-4V are achieving F1 score below 40%. In this work, we aim to enhance LMMs’ capability to reason over multiple images, which include different scenarios: (1) understanding actions in multiple video frames, (2) understanding differences between images, (3) high-level operations over multiple images. Achiam, J. (2023). GPT-4 Technical Report. &#8617; Team, C. (2023). Gemini: Understanding visual-language interactions. &#8617; Wang, L. (2024). Mementos: Benchmarking the understanding of image sequences. &#8617;]]></summary></entry></feed>